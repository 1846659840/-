{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-7xMyz1dHyO",
        "outputId": "4ea266f6-1a8d-43de-973a-efccc0e9174e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opencc\n",
            "  Downloading OpenCC-1.1.6-cp310-cp310-manylinux1_x86_64.whl (778 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.3/778.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencc\n",
            "Successfully installed opencc-1.1.6\n"
          ]
        }
      ],
      "source": [
        "!pip install opencc\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from opencc import OpenCC\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path =\"Hotel2000.xlsx\"\n",
        "df = pd.read_excel(file_path, header=None)\n",
        "cc = OpenCC('t2s')\n",
        "# 定義轉換函數\n",
        "def convert_to_simplified(text):\n",
        "    return cc.convert(text)\n",
        "# 將 0 列中的所有繁體字轉換成簡體字\n",
        "df[0] = df[0].apply(convert_to_simplified)"
      ],
      "metadata": {
        "id": "5-I6M3CWdMoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "CdZfMbLCdr0T",
        "outputId": "28e181a9-f520-4a12-ac5a-e0e732bf4ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                      0  \\\n",
              "0     距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...   \n",
              "1                          商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!   \n",
              "2     位置离我们单位很近,从价格来说,性价比很高.我要的大床房,168元,前台服务员态度很好,房间...   \n",
              "3     房间还算干净整洁,服务也可以,以这个价格来说,不错了,建议大家要定有窗户的房间,但是餐厅不够...   \n",
              "4     距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...   \n",
              "...                                                 ...   \n",
              "1995  尼斯酒店的几大特点：噪音大、环境差、配置低、服务效率低。如：1、隔壁歌厅的声音闹至午夜3点许...   \n",
              "1996                                     价格和房间严重不成比例的宾馆   \n",
              "1997  看照片觉得还挺不错的，又是4星级的，但入住以后除了后悔没有别的，房间挺大但空空的，早餐是有但...   \n",
              "1998  我们去盐城的时候那里的最低气温只有4度，晚上冷得要死，居然还不开空调，投诉到酒店客房部，得到...   \n",
              "1999  说实在的我很失望，之前看了其他人的点评后觉得还可以才去的，结果让我们大跌眼镜。我想这家酒店以...   \n",
              "\n",
              "                                                      1  \\\n",
              "0     距离 川沙 公路 较近 但是 公交 指示 不对 如果 是  蔡陆线 的 话 会 非常 麻烦 ...   \n",
              "1                 商务 大床房 房间 很大 床 有 2M 宽 整体 感觉 经济 实惠 不错    \n",
              "2     位置 离 我们 单位 很近 从 价格 来说 性价比 很高 我 要的 大床房  168 元 前...   \n",
              "3     房间 还算 干净 整洁 服务 也 可以 以 这个 价格 来说 不错 了 建议 大家 要 定有...   \n",
              "4     距离 川沙 公路 较近  但是 公交 指示 不对 如果 是 蔡陆线 的话  会 非常 麻烦 ...   \n",
              "...                                                 ...   \n",
              "1995  尼斯 酒店 的 几大 特点  噪音 大 环境 差 配置 低 服务 效率 低 如 隔壁 歌厅 ...   \n",
              "1996                              价格 和 房间 严重 不成 比例 的 宾馆   \n",
              "1997  看 照片 觉得 还 挺不错 的 又是 4星级 的 但 入住 以后 除了 后悔 没有 别的 房...   \n",
              "1998  我们 去 盐城 的 时候 那里 的 最低 气温 只有 4 度 晚上 冷得 要死 居然 还 不...   \n",
              "1999  说 实在 的 我 很 失望 之前 看了 其他人 的 点评 后 觉得 还可以 才 去 的 结果...   \n",
              "\n",
              "                                2              3                  4  5  \n",
              "0                      不对 麻烦 较为简单          不对 麻烦               较为简单  1  \n",
              "1                    大  经济 实惠 不错             NaN       大  经济 实惠 不错   1  \n",
              "2          高 一般 干净 太硬 骚扰 好 免费  不错     一般  太硬 骚扰      高 好  干净 免费  不错  1  \n",
              "3     干净 整洁  可以  不错 好 酸  不太方便 很远     酸  不太方便 很远   干净 整洁  可以  不错 好    1  \n",
              "4                  较近 不对 麻烦 建议 简单         不对 麻烦          较近 不对   简单  1  \n",
              "...                           ...            ...                ... ..  \n",
              "1995              大 差 低 闹 破旧 少 实惠     差 低 闹 破旧 少               大 实惠  0  \n",
              "1996                           严重             严重                NaN  0  \n",
              "1997                    不错 大 好 后悔             后悔             不错 大 好  0  \n",
              "1998                           可怜             可怜                NaN  0  \n",
              "1999                  失望 还可以 大跌眼镜        失望 大跌眼镜                还可以  0  \n",
              "\n",
              "[2000 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-161cb9e6-0c69-45b0-8e4d-e9cfd91f7193\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...</td>\n",
              "      <td>距离 川沙 公路 较近 但是 公交 指示 不对 如果 是  蔡陆线 的 话 会 非常 麻烦 ...</td>\n",
              "      <td>不对 麻烦 较为简单</td>\n",
              "      <td>不对 麻烦</td>\n",
              "      <td>较为简单</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!</td>\n",
              "      <td>商务 大床房 房间 很大 床 有 2M 宽 整体 感觉 经济 实惠 不错</td>\n",
              "      <td>大  经济 实惠 不错</td>\n",
              "      <td>NaN</td>\n",
              "      <td>大  经济 实惠 不错</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>位置离我们单位很近,从价格来说,性价比很高.我要的大床房,168元,前台服务员态度很好,房间...</td>\n",
              "      <td>位置 离 我们 单位 很近 从 价格 来说 性价比 很高 我 要的 大床房  168 元 前...</td>\n",
              "      <td>高 一般 干净 太硬 骚扰 好 免费  不错</td>\n",
              "      <td>一般  太硬 骚扰</td>\n",
              "      <td>高 好  干净 免费  不错</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>房间还算干净整洁,服务也可以,以这个价格来说,不错了,建议大家要定有窗户的房间,但是餐厅不够...</td>\n",
              "      <td>房间 还算 干净 整洁 服务 也 可以 以 这个 价格 来说 不错 了 建议 大家 要 定有...</td>\n",
              "      <td>干净 整洁  可以  不错 好 酸  不太方便 很远</td>\n",
              "      <td>酸  不太方便 很远</td>\n",
              "      <td>干净 整洁  可以  不错 好</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...</td>\n",
              "      <td>距离 川沙 公路 较近  但是 公交 指示 不对 如果 是 蔡陆线 的话  会 非常 麻烦 ...</td>\n",
              "      <td>较近 不对 麻烦 建议 简单</td>\n",
              "      <td>不对 麻烦</td>\n",
              "      <td>较近 不对   简单</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>尼斯酒店的几大特点：噪音大、环境差、配置低、服务效率低。如：1、隔壁歌厅的声音闹至午夜3点许...</td>\n",
              "      <td>尼斯 酒店 的 几大 特点  噪音 大 环境 差 配置 低 服务 效率 低 如 隔壁 歌厅 ...</td>\n",
              "      <td>大 差 低 闹 破旧 少 实惠</td>\n",
              "      <td>差 低 闹 破旧 少</td>\n",
              "      <td>大 实惠</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>价格和房间严重不成比例的宾馆</td>\n",
              "      <td>价格 和 房间 严重 不成 比例 的 宾馆</td>\n",
              "      <td>严重</td>\n",
              "      <td>严重</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>看照片觉得还挺不错的，又是4星级的，但入住以后除了后悔没有别的，房间挺大但空空的，早餐是有但...</td>\n",
              "      <td>看 照片 觉得 还 挺不错 的 又是 4星级 的 但 入住 以后 除了 后悔 没有 别的 房...</td>\n",
              "      <td>不错 大 好 后悔</td>\n",
              "      <td>后悔</td>\n",
              "      <td>不错 大 好</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>我们去盐城的时候那里的最低气温只有4度，晚上冷得要死，居然还不开空调，投诉到酒店客房部，得到...</td>\n",
              "      <td>我们 去 盐城 的 时候 那里 的 最低 气温 只有 4 度 晚上 冷得 要死 居然 还 不...</td>\n",
              "      <td>可怜</td>\n",
              "      <td>可怜</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>说实在的我很失望，之前看了其他人的点评后觉得还可以才去的，结果让我们大跌眼镜。我想这家酒店以...</td>\n",
              "      <td>说 实在 的 我 很 失望 之前 看了 其他人 的 点评 后 觉得 还可以 才 去 的 结果...</td>\n",
              "      <td>失望 还可以 大跌眼镜</td>\n",
              "      <td>失望 大跌眼镜</td>\n",
              "      <td>还可以</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-161cb9e6-0c69-45b0-8e4d-e9cfd91f7193')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-161cb9e6-0c69-45b0-8e4d-e9cfd91f7193 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-161cb9e6-0c69-45b0-8e4d-e9cfd91f7193');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba"
      ],
      "metadata": {
        "id": "2PhqqUgad8w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "custom_tokenized_data = []\n",
        "\n",
        "for text in df[2]:\n",
        "    # Convert non-string objects to strings\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    tokens = jieba.cut(text)\n",
        "    custom_tokenized_data.append(\" \".join(tokens))\n",
        "\n",
        "# Store tokenized text in a new column\n",
        "df['tokenized'] = custom_tokenized_data\n"
      ],
      "metadata": {
        "id": "dpZ0W8dofuWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56fddc6-d9ea-46ad-b649-0fb4fcdc8daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.908 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.908 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Convert all elements in the column to strings\n",
        "df[2] = df[2].apply(lambda x: str(x))\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Convert the text data into numerical features\n",
        "X = vectorizer.fit_transform(df[2])\n",
        "\n",
        "# Use df[5] as the target\n",
        "y = df[5]\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtTu3mbveaFr",
        "outputId": "cf554daa-48d8-46a7-cd6d-a7d6bf14941a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.89       298\n",
            "           1       0.94      0.81      0.87       302\n",
            "\n",
            "    accuracy                           0.88       600\n",
            "   macro avg       0.89      0.88      0.88       600\n",
            "weighted avg       0.89      0.88      0.88       600\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the misclassified samples\n",
        "misclassified_samples = (y_test != y_pred)\n",
        "\n",
        "# Get the original sentences of the misclassified samples\n",
        "misclassified_sentences = X_test[misclassified_samples].toarray()\n",
        "misclassified_sentences = vectorizer.inverse_transform(misclassified_sentences)\n",
        "\n",
        "# Print the misclassified sentences\n",
        "for i, sentence in enumerate(misclassified_sentences):\n",
        "    print(f\"Misclassified sentence {i+1}: {' '.join(sentence)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DmEKJuzs70K",
        "outputId": "63f5a223-79f3-41c1-fc35-95b54abe5b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misclassified sentence 1: 感谢 认真 负责\n",
            "Misclassified sentence 2: 死板 被动\n",
            "Misclassified sentence 3: 上选 不便 小有名气 方便 没有\n",
            "Misclassified sentence 4: 还好\n",
            "Misclassified sentence 5: 不错\n",
            "Misclassified sentence 6: 好吃 安静 舒服 问题\n",
            "Misclassified sentence 7: 一般\n",
            "Misclassified sentence 8: 不错 冷淡 生硬\n",
            "Misclassified sentence 9: 不错 继续\n",
            "Misclassified sentence 10: 缺乏\n",
            "Misclassified sentence 11: 缺乏\n",
            "Misclassified sentence 12: 不行 还好\n",
            "Misclassified sentence 13: 老化\n",
            "Misclassified sentence 14: 有才\n",
            "Misclassified sentence 15: 一般\n",
            "Misclassified sentence 16: 不如 不错 温馨\n",
            "Misclassified sentence 17: 上选 不便 小有名气 方便 没有\n",
            "Misclassified sentence 18: 所值 物有\n",
            "Misclassified sentence 19: 即使 哈哈 很多 忘记 立即 重视\n",
            "Misclassified sentence 20: 一般 不好 不实用 严重 安静 方便\n",
            "Misclassified sentence 21: 不是很突出\n",
            "Misclassified sentence 22: 吝啬\n",
            "Misclassified sentence 23: 不理想 还可以\n",
            "Misclassified sentence 24: 不错 忍受 改善 无法\n",
            "Misclassified sentence 25: \n",
            "Misclassified sentence 26: 不小 不错 方便\n",
            "Misclassified sentence 27: 不好吃 便利\n",
            "Misclassified sentence 28: 牛气\n",
            "Misclassified sentence 29: 不错\n",
            "Misclassified sentence 30: 没有\n",
            "Misclassified sentence 31: 最好的 热闹 绝对的 豪华\n",
            "Misclassified sentence 32: 一般\n",
            "Misclassified sentence 33: 无法 没有 骚扰\n",
            "Misclassified sentence 34: 一般 不如 优点 好吃 容忍 无法\n",
            "Misclassified sentence 35: 不能 不错 失望\n",
            "Misclassified sentence 36: 不方便 不负责任 不错\n",
            "Misclassified sentence 37: 最好的 热闹 绝对的 豪华\n",
            "Misclassified sentence 38: 不值 不够 不错 详细\n",
            "Misclassified sentence 39: 有才\n",
            "Misclassified sentence 40: 老旧 该死 适合\n",
            "Misclassified sentence 41: 不小心 完璧归赵 闹中取静\n",
            "Misclassified sentence 42: 牛气\n",
            "Misclassified sentence 43: 不明白\n",
            "Misclassified sentence 44: 扑克脸 方便\n",
            "Misclassified sentence 45: 不干净 不错 可以\n",
            "Misclassified sentence 46: 不满意 中规中矩\n",
            "Misclassified sentence 47: 不知道 不错 为什么\n",
            "Misclassified sentence 48: 没有\n",
            "Misclassified sentence 49: 不是很突出\n",
            "Misclassified sentence 50: 一般 值得住\n",
            "Misclassified sentence 51: 不明白\n",
            "Misclassified sentence 52: 不好 不错\n",
            "Misclassified sentence 53: 满意 简单\n",
            "Misclassified sentence 54: 不快 优点 笑脸 美丽\n",
            "Misclassified sentence 55: 不足 效率低 迟钝\n",
            "Misclassified sentence 56: 一般\n",
            "Misclassified sentence 57: 优越 破败\n",
            "Misclassified sentence 58: 一般 不太好 陈旧\n",
            "Misclassified sentence 59: 宽敞 尚可\n",
            "Misclassified sentence 60: 不知为何\n",
            "Misclassified sentence 61: 不小 不错 方便\n",
            "Misclassified sentence 62: 不佳 引发 确实 而已\n",
            "Misclassified sentence 63: 优美\n",
            "Misclassified sentence 64: 便宜 安静 迷糊\n",
            "Misclassified sentence 65: 建议 挥之不去 推荐 无当 污渍 豪华 酒气\n",
            "Misclassified sentence 66: 不方便\n",
            "Misclassified sentence 67: 不像话 不错 可惜 怪不得 歧视 谢绝 郁闷\n",
            "Misclassified sentence 68: 满意 简单\n",
            "Misclassified sentence 69: 简陋\n",
            "Misclassified sentence 70: 不符 异味 绝佳 陈旧\n",
            "Misclassified sentence 71: 所值 物有\n",
            "Misclassified sentence 72: 不方便\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pPHYmcR7WntX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = []\n",
        "for text in df[0]:\n",
        "    tokens = jieba.cut(text)\n",
        "    tokenized_text.append(\" \".join(tokens))\n",
        "\n",
        "# 将分词后的文本存储在一个新的列中\n",
        "df['tokenized'] = tokenized_text\n",
        "\n",
        "df['tokenized'] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEdp624ahTt7",
        "outputId": "8a9552ec-1c82-400f-90b4-d812cf886108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       距离 川沙 公路 较近 , 但是 公交 指示 不 对 , 如果 是 \" 蔡陆线 \" 的话 ,...\n",
              "1            商务 大床 房 ， 房间 很大 ， 床有 2M 宽 ， 整体 感觉 经济 实惠 不错 !\n",
              "2       位置 离 我们 单位 很近 , 从 价格 来说 , 性价比 很 高 . 我要 的 大床 房 ...\n",
              "3       房间 还 算 干净 整洁 , 服务 也 可以 , 以 这个 价格 来说 , 不错 了 , 建...\n",
              "4       距离 川沙 公路 较近 , 但是 公交 指示 不 对 , 如果 是 \" 蔡陆线 \" 的话 ,...\n",
              "                              ...                        \n",
              "1995    尼斯 酒店 的 几大 特点 ： 噪音 大 、 环境 差 、 配置 低 、 服务 效率 低 。...\n",
              "1996                                 价格 和 房间 严重 不成比例 的 宾馆\n",
              "1997    看 照片 觉得 还 挺不错 的 ， 又 是 4 星级 的 ， 但 入住 以后 除了 后悔 没...\n",
              "1998    我们 去 盐城 的 时候 那里 的 最低气温 只有 4 度 ， 晚上 冷得 要死 ， 居然 ...\n",
              "1999    说 实在 的 我 很 失望 ， 之前 看 了 其他人 的 点评 后 觉得 还 可以 才 去 ...\n",
              "Name: tokenized, Length: 2000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 读取停用词文件，将停用词存储在一个集合中\n",
        "stopwords_path = 'baidu_stopwords.txt'\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().splitlines())\n",
        "\n",
        "# 对 df['tokenized'] 列中的文本进行迭代，并删除其中的停用词\n",
        "cleaned_text = []\n",
        "for text in df['tokenized']:\n",
        "    tokens = text.split()\n",
        "    cleaned_tokens = [token for token in tokens if token not in stopwords]\n",
        "    cleaned_text.append(\" \".join(cleaned_tokens))\n",
        "\n",
        "# 将删除停用词后的文本存储在一个新的列中\n",
        "df['cleaned'] = cleaned_text"
      ],
      "metadata": {
        "id": "ypMtG6f9hsQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_dict_path = 'output.txt'\n",
        "jieba.load_userdict(user_dict_path)\n",
        "\n",
        "# 使用自定义词典对 df['cleaned'] 列中的文本进行分词\n",
        "custom_tokenized_data = []\n",
        "for text in df['cleaned']:\n",
        "    tokens = jieba.cut(text)\n",
        "    custom_tokenized_data.append(\" \".join(tokens))\n",
        "\n",
        "# 将使用自定义词典分词后的文本存储在一个新的列中\n",
        "df['custom_tokenized'] = custom_tokenized_data"
      ],
      "metadata": {
        "id": "h1-Oo5gXhvsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "cleaned_tokens_without_punctuation_and_digits = []\n",
        "\n",
        "for text in df['custom_tokenized']:\n",
        "    # 使用unicodedata库去除所有标点符号和数字\n",
        "    cleaned_text = ''.join([char for char in text if not unicodedata.category(char).startswith('P') and not char.isdigit()])\n",
        "    cleaned_tokens_without_punctuation_and_digits.append(cleaned_text)\n",
        "\n",
        "# 将去除标点符号和数字后的文本存储在一个新的列中\n",
        "df['final_cleaned'] = cleaned_tokens_without_punctuation_and_digits\n",
        "df['final_cleaned']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coJjcm5Ih0lR",
        "outputId": "3e24cfab-5fa6-4d75-ebc6-8cd17ff2c7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       距离   川沙   公路   较近      公交   指示   不         蔡陆线...\n",
              "1       商务   大床   房      房间   很大      床有   M   宽      ...\n",
              "2       位置   单位   很近      价格   来说      性价比   很   高    ...\n",
              "3       房间   还   算   干净   整洁      服务      价格   来说     ...\n",
              "4       距离   川沙   公路   较近      公交   指示   不         蔡陆线...\n",
              "                              ...                        \n",
              "1995    尼斯   酒店   几大      噪音   大      环境   差      配置  ...\n",
              "1996                                  价格   房间   不成比例   宾馆\n",
              "1997    看   照片   还   挺不错         星级      入住   后悔      ...\n",
              "1998    去   盐城   最低气温      度      晚上   冷得   要死      居然...\n",
              "1999    说   实在   很   失望      看   其他人   点评   后   还   才 ...\n",
              "Name: final_cleaned, Length: 2000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tokens_without_uppercase = []\n",
        "\n",
        "for text in df['final_cleaned']:\n",
        "    # 去除大写英文字母\n",
        "    cleaned_text = ''.join([char for char in text if not char.isupper()])\n",
        "    cleaned_tokens_without_uppercase.append(cleaned_text)\n",
        "\n",
        "# 将去除大写英文字母后的文本存储在一个新的列中\n",
        "df['final_cleaned_no_upper'] = cleaned_tokens_without_uppercase\n",
        "df['final_cleaned_no_upper']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcHlqloGh4mZ",
        "outputId": "3cb15e9b-43c8-4248-f23b-86bcff9d5ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       距离   川沙   公路   较近      公交   指示   不         蔡陆线...\n",
              "1       商务   大床   房      房间   很大      床有      宽      整...\n",
              "2       位置   单位   很近      价格   来说      性价比   很   高    ...\n",
              "3       房间   还   算   干净   整洁      服务      价格   来说     ...\n",
              "4       距离   川沙   公路   较近      公交   指示   不         蔡陆线...\n",
              "                              ...                        \n",
              "1995    尼斯   酒店   几大      噪音   大      环境   差      配置  ...\n",
              "1996                                  价格   房间   不成比例   宾馆\n",
              "1997    看   照片   还   挺不错         星级      入住   后悔      ...\n",
              "1998    去   盐城   最低气温      度      晚上   冷得   要死      居然...\n",
              "1999    说   实在   很   失望      看   其他人   点评   后   还   才 ...\n",
              "Name: final_cleaned_no_upper, Length: 2000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perturb_sentence(sentence, num_perturbations):\n",
        "    perturbed_sentences = []\n",
        "    words = sentence.split()\n",
        "    num_words = len(words)\n",
        "    \n",
        "    for _ in range(num_perturbations):\n",
        "        perturbed_words = words.copy()\n",
        "        num_words_to_remove = random.randint(1, num_words//2)\n",
        "        indices_to_remove = random.sample(range(num_words), num_words_to_remove)\n",
        "        \n",
        "        for index in sorted(indices_to_remove, reverse=True):\n",
        "            del perturbed_words[index]\n",
        "        \n",
        "        perturbed_sentence = ' '.join(perturbed_words)\n",
        "        perturbed_sentences.append(perturbed_sentence)\n",
        "    \n",
        "    return perturbed_sentences"
      ],
      "metadata": {
        "id": "G77yl-_Ch-co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "data=[]\n",
        "for k in tqdm(range(len(df))):\n",
        "    sentence = df['final_cleaned_no_upper'].iloc[k]\n",
        "    num_perturbations = 1000\n",
        "    perturbed_sentences = perturb_sentence(sentence, num_perturbations)\n",
        "    \n",
        "    perturbed_sentences_tfidf = vectorizer.transform(perturbed_sentences)\n",
        "    svm_classifier = SVC(probability=True)\n",
        "    svm_classifier.fit(X_train, y_train)\n",
        "    predictions_proba = svm_classifier.predict_proba(perturbed_sentences_tfidf)\n",
        "    predictions_proba_class_1 = predictions_proba[:, 0]\n",
        "#     for i, prediction_proba in enumerate(predictions_proba_class_1[:10]):\n",
        "#         print(f\"预测为1的概率 {i + 1}: {prediction_proba}\")\n",
        "    predictions_proba_class_1_array = np.array(predictions_proba_class_1)\n",
        "    \n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    from scipy.spatial.distance import cdist\n",
        "    original_sentence_tfidf = vectorizer.transform([sentence])\n",
        "    cosine_similarities = cosine_similarity(original_sentence_tfidf, perturbed_sentences_tfidf)\n",
        "    kernel_width = 0.25\n",
        "    rbf_similarities = np.exp(-0.5 * (1 - cosine_similarities) ** 2 / kernel_width**2)\n",
        "#     for i, similarity in enumerate(rbf_similarities[0][:10]):\n",
        "#         print(f\"调整后的相似性 {i + 1}: {similarity}\")\n",
        "    rbf_similarities_array = np.array(rbf_similarities)\n",
        "    \n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    sentence_tokens = sentence.split()\n",
        "    attributes_matrix = np.zeros((len(perturbed_sentences), len(sentence_tokens)))\n",
        "    for i, perturbed_sentence in enumerate(perturbed_sentences):\n",
        "        perturbed_sentence_tokens = perturbed_sentence.split()\n",
        "        for j, original_token in enumerate(sentence_tokens):\n",
        "            if original_token in perturbed_sentence_tokens:\n",
        "                attributes_matrix[i, j] = 1\n",
        "    rbf_similarities_array = np.array(rbf_similarities[0])\n",
        "    linear_regression = LinearRegression()\n",
        "    linear_regression.fit(attributes_matrix, predictions_proba_class_1_array, sample_weight=rbf_similarities_array)\n",
        "    \n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.multioutput import MultiOutputRegressor\n",
        "    regr_rf=RandomForestRegressor()\n",
        "    regr_rf.fit(X=attributes_matrix, y=predictions_proba_class_1_array,sample_weight=rbf_similarities_array)\n",
        "    importances1 = regr_rf.feature_importances_\n",
        "    \n",
        "    from sklearn import tree\n",
        "    treemodel=tree.DecisionTreeRegressor(max_depth=5)\n",
        "    treemodel.fit(attributes_matrix,predictions_proba_class_1_array,sample_weight=rbf_similarities_array)\n",
        "    importances = treemodel.feature_importances_\n",
        "    \n",
        "    import xgboost as xgb\n",
        "    xgbrModel=xgb.XGBRegressor()\n",
        "    xgbrModel.fit(attributes_matrix,predictions_proba_class_1_array,sample_weight=rbf_similarities_array)\n",
        "    importances2=xgbrModel.feature_importances_\n",
        "    \n",
        "    \n",
        "    word_coef_pairs = list(zip(sentence_tokens, linear_regression.coef_))\n",
        "    sorted_word_coef_pairs = sorted([(word, abs(coef)) for word, coef in word_coef_pairs], key=lambda x: x[1], reverse=True)\n",
        "    if pd.isna(df.iloc[k, 2]):\n",
        "        linear_score=0\n",
        "        importance0_score=0\n",
        "        importance1_score=0\n",
        "        importance2_score=0\n",
        "        linear_most_important_words='该栏位没有词'\n",
        "        importance0_most_important_words='该栏位没有词'\n",
        "        importance1_most_important_words='该栏位没有词'\n",
        "        importance2_most_important_words='该栏位没有词'\n",
        "    else:\n",
        "        value_of_df_4 = df.iloc[k, 2]\n",
        "        tokens_of_df_4 = value_of_df_4.split()\n",
        "        num_of_words = len(tokens_of_df_4)\n",
        "        most_important_words = [pair[0] for pair in sorted_word_coef_pairs[:num_of_words]]\n",
        "        matched_words = [word for word in tokens_of_df_4 if word in most_important_words]\n",
        "        linear_score = len(matched_words) / num_of_words * 100\n",
        "        linear_most_important_words=','.join(most_important_words)\n",
        "\n",
        "        words = [pair[0] for pair in sorted_word_coef_pairs]\n",
        "        word_importance_pairs = list(zip(words, importances))\n",
        "        word_importance1_pairs = list(zip(words, importances1))\n",
        "        word_importance2_pairs = list(zip(words, importances2))\n",
        "        sorted_word_importance_pairs = sorted(word_importance_pairs, key=lambda x: -abs(x[1]))\n",
        "        sorted_word_importance1_pairs = sorted(word_importance1_pairs, key=lambda x: -abs(x[1]))\n",
        "        sorted_word_importance2_pairs = sorted(word_importance2_pairs, key=lambda x: -abs(x[1]))\n",
        "        sorted_word_importance_pairs_list = [\n",
        "            sorted_word_importance_pairs,\n",
        "            sorted_word_importance1_pairs,\n",
        "            sorted_word_importance2_pairs\n",
        "        ]\n",
        "        results = []\n",
        "        for sorted_word_importance_pairs in sorted_word_importance_pairs_list:\n",
        "            most_important_words = [pair[0] for pair in sorted_word_importance_pairs[:num_of_words]]\n",
        "            matched_words = [word for word in tokens_of_df_4 if word in most_important_words]\n",
        "            score = len(matched_words) / num_of_words * 100\n",
        "            results.append((score, most_important_words))\n",
        "        importance0_score, importance0_most_important_words=results[0]\n",
        "        importance0_most_important_words=','.join(importance0_most_important_words)\n",
        "        importance1_score, importance1_most_important_words=results[1]\n",
        "        importance1_most_important_words=','.join(importance1_most_important_words)\n",
        "        importance2_score, importance2_most_important_words=results[2]\n",
        "        importance2_most_important_words=','.join(importance2_most_important_words)\n",
        "    data.append([linear_score,linear_most_important_words,\n",
        "                importance0_score,importance0_most_important_words,\n",
        "                importance1_score, importance1_most_important_words,\n",
        "                importance2_score,importance2_most_important_words])\n",
        "\n",
        "result_df=pd.DataFrame(data=data,columns=['linear_score','linear_most_important_words',\n",
        "                               'importance0_score','importance0_most_important_words',\n",
        "                               'importance1_score','importance1_most_important_words',\n",
        "                               'importance2_score','importance2_most_important_words']) \n",
        "result_df.to_csv('results.csv',index=False,encoding='utf_8_sig')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqREqaLjiDgv",
        "outputId": "aa328738-8902-4372-8a2b-6b09755e22ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [49:15<00:00,  1.48s/it]\n"
          ]
        }
      ]
    }
  ]
}